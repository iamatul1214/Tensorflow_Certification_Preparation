{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamatul1214/Tensorflow_Certification_Preparation/blob/main/05_TransferLearning_using_tensorflow_part2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MaoulC9J1qc"
      },
      "source": [
        "### Here we will perform transfer learning with fine tuning of our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T5TIrBWJiyI",
        "outputId": "3b2160e0-defb-4758-95eb-76378fa653fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/Tensorflow certifications work\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "Root = \"/content/drive/MyDrive/Tensorflow certifications work\"\n",
        "!pwd\n",
        "os.chdir(Root)\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFflacULcsJ"
      },
      "source": [
        "## We have created a helper function which includes all those small functions we worked in previous modules.\n",
        "\n",
        "### We can import the functions form the helper.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzeJdPiZJxsJ",
        "outputId": "ac6ce481-91b8-4190-a1f4-c73e359b4f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "helper.py exists already, hence skipping downloading..\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  if os.path.exists(os.path.join(os.getcwd(),\"Helper.py\")):\n",
        "    print(\"helper.py exists already, hence skipping downloading..\")\n",
        "  else:\n",
        "    !wget https://raw.githubusercontent.com/iamatul1214/Tensorflow_Certification_Preparation/main/Helper.py\n",
        "    print(\"Downloaded helper.py successfully\")\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyQyGXvDMlHe"
      },
      "outputs": [],
      "source": [
        "from Helper import create_tensorboard_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czXEGN1GarSE"
      },
      "source": [
        "## Let's get some data\n",
        "\n",
        "### This time we will see how we can use the pretrained models frmo `tf.keras.applications` and apply them to our own problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tXGmn2igA_8",
        "outputId": "a38cff03-58cd-486e-d0f0-67dd5ff87ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data set is already unzipped and present\n"
          ]
        }
      ],
      "source": [
        "## Getting 10% data of of 10 classes of food\n",
        "from Helper import unzip_data\n",
        "try:\n",
        "  if os.path.exists(os.path.join(os.getcwd(),\"10_food_classes_10_percent.zip\")):\n",
        "    print(f\"data set is already unzipped and present\")\n",
        "  else:\n",
        "    print(\"Downloading the data and unzipping it...\")\n",
        "    !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "    ### unzipping data\n",
        "    print(\"Unzipping the data\")\n",
        "    unzip_data(\"10_food_classes_10_percent.zip\")\n",
        "\n",
        "\n",
        "    \n",
        "except Exception as e:\n",
        "  print(\"data not found in the directory... Hence downloading.....\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC9sX55zeyPk"
      },
      "outputs": [],
      "source": [
        "## creating train and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train\"\n",
        "test_dir = \"10_food_classes_10_percent/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgWRCBYQdZLx",
        "outputId": "0f19f237-81ca-48ba-a2f1-886970d62cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n"
          ]
        }
      ],
      "source": [
        "## Let's walkthrough the data\n",
        "from Helper import walk_through_dir\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkaaKep2jBRA",
        "outputId": "7bc4f03f-a847-4cef-9d7f-476b50766530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "## This time we will not use the ImageDataGenerator, we will use even a smarter version of that.\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory = train_dir,\n",
        "                                                                            image_size = IMG_SIZE,\n",
        "                                                                            label_mode = \"categorical\",\n",
        "                                                                            batch_size = BATCH_SIZE)\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory = test_dir,\n",
        "                                                                image_size = IMG_SIZE,\n",
        "                                                                label_mode = \"categorical\",\n",
        "                                                                batch_size = BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWCedF_moack",
        "outputId": "03ad1401-3e85-48ef-b7cc-dc7332b5c57d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_10_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYN1d-u9o29D",
        "outputId": "722beac3-a2e8-43a5-9981-8c5d213d1a5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_10_percent.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97YRzUVcrDf7",
        "outputId": "1acfbf62-aadf-4c49-b7a9-a014f9f55954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images = [[[[6.99719391e+01 5.12125320e+01 2.63330669e+01]\n",
            "   [8.29800720e+01 5.84636497e+01 3.21310616e+01]\n",
            "   [9.64749680e+01 6.43188782e+01 3.56200562e+01]\n",
            "   ...\n",
            "   [1.39580536e+02 8.15805359e+01 3.35805321e+01]\n",
            "   [1.36928558e+02 7.89285583e+01 3.09285583e+01]\n",
            "   [1.36093277e+02 7.80932846e+01 3.00932808e+01]]\n",
            "\n",
            "  [[8.54062500e+01 5.64336739e+01 2.87893810e+01]\n",
            "   [9.46787338e+01 6.16811256e+01 3.38262138e+01]\n",
            "   [9.73990707e+01 5.94352684e+01 2.97946415e+01]\n",
            "   ...\n",
            "   [1.39167877e+02 8.11678696e+01 3.31678734e+01]\n",
            "   [1.36201050e+02 7.82010498e+01 3.02010498e+01]\n",
            "   [1.37140656e+02 7.91406631e+01 3.11406593e+01]]\n",
            "\n",
            "  [[9.40644150e+01 5.58411980e+01 2.65353947e+01]\n",
            "   [9.89722595e+01 5.87490425e+01 2.74432392e+01]\n",
            "   [9.90797195e+01 5.71050720e+01 2.41304188e+01]\n",
            "   ...\n",
            "   [1.41520065e+02 8.35200653e+01 3.55200691e+01]\n",
            "   [1.36283951e+02 7.82839584e+01 3.02839565e+01]\n",
            "   [1.36801712e+02 7.88017044e+01 3.08017063e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[7.23370743e+01 6.03370705e+01 4.43370705e+01]\n",
            "   [7.39285736e+01 6.19285736e+01 4.59285736e+01]\n",
            "   [7.70655289e+01 6.40655289e+01 4.70655251e+01]\n",
            "   ...\n",
            "   [1.35518188e+02 1.28732452e+02 1.18303932e+02]\n",
            "   [1.26945641e+02 1.19945641e+02 1.09945641e+02]\n",
            "   [1.15463867e+02 1.08463867e+02 9.84638672e+01]]\n",
            "\n",
            "  [[7.42165222e+01 6.22165222e+01 4.62165222e+01]\n",
            "   [7.42165222e+01 6.22165222e+01 4.62165222e+01]\n",
            "   [7.59536057e+01 6.29536018e+01 4.59536018e+01]\n",
            "   ...\n",
            "   [1.13602448e+02 1.06602448e+02 9.66024475e+01]\n",
            "   [1.00457428e+02 9.34574280e+01 8.34574280e+01]\n",
            "   [8.63167114e+01 7.93167114e+01 6.97497559e+01]]\n",
            "\n",
            "  [[7.57388611e+01 6.37388611e+01 4.77388611e+01]\n",
            "   [7.50527725e+01 6.30527725e+01 4.70527725e+01]\n",
            "   [7.52051773e+01 6.22051811e+01 4.52051811e+01]\n",
            "   ...\n",
            "   [8.89974823e+01 8.19974823e+01 7.31585770e+01]\n",
            "   [7.60717239e+01 6.90717239e+01 6.05494499e+01]\n",
            "   [6.43505478e+01 5.73505440e+01 4.93505440e+01]]]\n",
            "\n",
            "\n",
            " [[[1.64775513e+02 1.41418365e+02 1.07418373e+02]\n",
            "   [1.93107147e+02 1.73897964e+02 1.40943878e+02]\n",
            "   [1.91913269e+02 1.81071442e+02 1.45352051e+02]\n",
            "   ...\n",
            "   [1.84714279e+02 1.82714279e+02 1.83714279e+02]\n",
            "   [1.83285706e+02 1.81285706e+02 1.82285706e+02]\n",
            "   [1.81127533e+02 1.79127533e+02 1.80127533e+02]]\n",
            "\n",
            "  [[1.74795914e+02 1.51933670e+02 1.17887756e+02]\n",
            "   [1.88913254e+02 1.72637756e+02 1.36704086e+02]\n",
            "   [1.87474487e+02 1.75071426e+02 1.39872452e+02]\n",
            "   ...\n",
            "   [1.88071426e+02 1.86071426e+02 1.87071426e+02]\n",
            "   [1.85999985e+02 1.83999985e+02 1.84999985e+02]\n",
            "   [1.83739761e+02 1.81739761e+02 1.82739761e+02]]\n",
            "\n",
            "  [[1.64755096e+02 1.44525513e+02 1.08030609e+02]\n",
            "   [1.80591827e+02 1.65178574e+02 1.26877548e+02]\n",
            "   [1.74836731e+02 1.62622452e+02 1.25265305e+02]\n",
            "   ...\n",
            "   [1.89000000e+02 1.87000000e+02 1.88000000e+02]\n",
            "   [1.87658142e+02 1.85658142e+02 1.86658142e+02]\n",
            "   [1.85000000e+02 1.83000000e+02 1.84000000e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.91147842e+02 1.95290665e+02 1.88862076e+02]\n",
            "   [1.96770294e+02 2.00913116e+02 1.94913116e+02]\n",
            "   [2.01423386e+02 2.05948883e+02 2.01397842e+02]\n",
            "   ...\n",
            "   [2.51642792e+02 2.55000000e+02 2.54214264e+02]\n",
            "   [2.50928558e+02 2.54928558e+02 2.53928558e+02]\n",
            "   [2.50000000e+02 2.54000000e+02 2.53000000e+02]]\n",
            "\n",
            "  [[1.58285309e+02 1.56499542e+02 1.44785187e+02]\n",
            "   [1.65218994e+02 1.62504639e+02 1.53718872e+02]\n",
            "   [1.75555756e+02 1.75754684e+02 1.66397491e+02]\n",
            "   ...\n",
            "   [2.51642792e+02 2.55000000e+02 2.54214264e+02]\n",
            "   [2.50928558e+02 2.54928558e+02 2.53928558e+02]\n",
            "   [2.50000000e+02 2.54000000e+02 2.53000000e+02]]\n",
            "\n",
            "  [[1.64642746e+02 1.59357040e+02 1.46071335e+02]\n",
            "   [1.70142746e+02 1.64857040e+02 1.51571335e+02]\n",
            "   [1.75020309e+02 1.70163177e+02 1.58443802e+02]\n",
            "   ...\n",
            "   [2.51642792e+02 2.55000000e+02 2.54214264e+02]\n",
            "   [2.50928558e+02 2.54928558e+02 2.53928558e+02]\n",
            "   [2.50000000e+02 2.54000000e+02 2.53000000e+02]]]\n",
            "\n",
            "\n",
            " [[[4.50561218e+01 3.10561237e+01 3.10561237e+01]\n",
            "   [4.71683693e+01 3.51683693e+01 3.51683693e+01]\n",
            "   [5.13622437e+01 3.93622437e+01 3.93622437e+01]\n",
            "   ...\n",
            "   [9.42143326e+01 1.01642860e+02 1.09000069e+02]\n",
            "   [1.06326408e+02 1.10540733e+02 1.19469292e+02]\n",
            "   [9.54849167e+01 1.04025757e+02 1.13255333e+02]]\n",
            "\n",
            "  [[5.13061218e+01 3.93061218e+01 3.93061218e+01]\n",
            "   [5.56428604e+01 4.36428604e+01 4.36428604e+01]\n",
            "   [5.72142868e+01 4.52142868e+01 4.52142868e+01]\n",
            "   ...\n",
            "   [9.43979187e+01 1.03683594e+02 1.10112228e+02]\n",
            "   [9.44898224e+01 1.01622498e+02 1.11556160e+02]\n",
            "   [9.38877258e+01 1.04836807e+02 1.13505142e+02]]\n",
            "\n",
            "  [[5.69234695e+01 4.53520393e+01 4.55663261e+01]\n",
            "   [5.54846916e+01 4.39132652e+01 4.41275482e+01]\n",
            "   [5.86224518e+01 4.86683693e+01 4.80969391e+01]\n",
            "   ...\n",
            "   [9.49540634e+01 1.04336678e+02 1.12831627e+02]\n",
            "   [9.44949112e+01 1.04724541e+02 1.13867386e+02]\n",
            "   [9.25101852e+01 1.08148033e+02 1.16576576e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.46531334e+01 5.62246056e+01 5.34388695e+01]\n",
            "   [3.58572540e+01 6.00154800e+01 5.63011703e+01]\n",
            "   [2.89285927e+01 5.32858009e+01 4.95000648e+01]\n",
            "   ...\n",
            "   [1.34357208e+02 1.21214264e+02 1.33785736e+02]\n",
            "   [1.37142822e+02 1.21142822e+02 1.32142822e+02]\n",
            "   [1.35857086e+02 1.17857086e+02 1.29857086e+02]]\n",
            "\n",
            "  [[4.00459137e+01 6.50459137e+01 6.20459137e+01]\n",
            "   [3.54082108e+01 6.13418846e+01 5.83418846e+01]\n",
            "   [3.26684151e+01 5.86684151e+01 5.56684151e+01]\n",
            "   ...\n",
            "   [1.31173615e+02 1.18030678e+02 1.30602142e+02]\n",
            "   [1.37989792e+02 1.21989792e+02 1.32989792e+02]\n",
            "   [1.39688721e+02 1.21688713e+02 1.33688721e+02]]\n",
            "\n",
            "  [[3.66837349e+01 6.26837349e+01 5.96837349e+01]\n",
            "   [3.51683006e+01 6.11683006e+01 5.81683006e+01]\n",
            "   [3.07857132e+01 5.91428566e+01 5.53571434e+01]\n",
            "   ...\n",
            "   [1.33489792e+02 1.20346848e+02 1.32918320e+02]\n",
            "   [1.38336777e+02 1.22336769e+02 1.33336777e+02]\n",
            "   [1.38199112e+02 1.20199112e+02 1.32199112e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[4.70969391e+01 2.90969391e+01 7.14285731e-01]\n",
            "   [5.75255089e+01 3.95255127e+01 3.77551079e+00]\n",
            "   [6.63061218e+01 5.04438744e+01 1.60204067e+01]\n",
            "   ...\n",
            "   [1.42438873e+02 1.22219398e+02 1.02443901e+02]\n",
            "   [1.43668365e+02 1.21000000e+02 9.60050507e+01]\n",
            "   [1.47612305e+02 1.21943932e+02 9.31836624e+01]]\n",
            "\n",
            "  [[9.14540787e+01 7.25255051e+01 2.91683655e+01]\n",
            "   [8.80204086e+01 7.10153122e+01 2.78010197e+01]\n",
            "   [9.66734695e+01 8.15306168e+01 4.07295914e+01]\n",
            "   ...\n",
            "   [1.65602081e+02 1.41744888e+02 1.17959160e+02]\n",
            "   [1.65076538e+02 1.37923462e+02 1.10505066e+02]\n",
            "   [1.68357178e+02 1.39285751e+02 1.07142891e+02]]\n",
            "\n",
            "  [[1.46729599e+02 1.27158173e+02 7.68010178e+01]\n",
            "   [1.42913269e+02 1.25000000e+02 7.58571472e+01]\n",
            "   [1.40290817e+02 1.23908173e+02 7.65306168e+01]\n",
            "   ...\n",
            "   [1.65831650e+02 1.36688751e+02 1.11499931e+02]\n",
            "   [1.68428589e+02 1.36255112e+02 1.05341820e+02]\n",
            "   [1.70433685e+02 1.38076538e+02 1.04300972e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.57499969e+02 1.35499969e+02 7.84999619e+01]\n",
            "   [1.57683670e+02 1.34683670e+02 8.06836624e+01]\n",
            "   [1.61096909e+02 1.37882629e+02 8.83111954e+01]\n",
            "   ...\n",
            "   [1.07857178e+02 8.10714722e+01 2.56428223e+01]\n",
            "   [1.11699043e+02 8.49133377e+01 2.95970154e+01]\n",
            "   [1.13357056e+02 8.55713501e+01 3.44998779e+01]]\n",
            "\n",
            "  [[1.55285736e+02 1.33285736e+02 7.62857285e+01]\n",
            "   [1.58709229e+02 1.35709229e+02 8.34337463e+01]\n",
            "   [1.57112213e+02 1.33168320e+02 8.54540787e+01]\n",
            "   ...\n",
            "   [1.12857178e+02 8.68571777e+01 2.64285278e+01]\n",
            "   [1.11290810e+02 8.52908096e+01 2.47091312e+01]\n",
            "   [1.13734634e+02 8.76632233e+01 2.89488659e+01]]\n",
            "\n",
            "  [[1.55071442e+02 1.33071442e+02 7.60714417e+01]\n",
            "   [1.58974503e+02 1.35974503e+02 8.38316422e+01]\n",
            "   [1.52352066e+02 1.28352066e+02 8.10561447e+01]\n",
            "   ...\n",
            "   [1.15795959e+02 8.67959595e+01 2.69490089e+01]\n",
            "   [1.14188812e+02 8.61632919e+01 2.32398396e+01]\n",
            "   [1.24545982e+02 9.55459824e+01 3.55459862e+01]]]\n",
            "\n",
            "\n",
            " [[[2.41872452e+02 2.36801025e+02 2.28158157e+02]\n",
            "   [2.35188766e+02 2.37974487e+02 2.26903061e+02]\n",
            "   [2.17290817e+02 2.30933670e+02 2.25219391e+02]\n",
            "   ...\n",
            "   [2.06142883e+02 5.71406722e-01 1.23571424e+01]\n",
            "   [2.06357147e+02 3.57142806e-01 1.23571424e+01]\n",
            "   [2.06357147e+02 3.57142806e-01 1.23571424e+01]]\n",
            "\n",
            "  [[2.40591843e+02 2.39357147e+02 2.29760193e+02]\n",
            "   [2.29709183e+02 2.35423462e+02 2.25219376e+02]\n",
            "   [2.10341827e+02 2.22826523e+02 2.19714279e+02]\n",
            "   ...\n",
            "   [2.04000000e+02 0.00000000e+00 1.10000000e+01]\n",
            "   [2.05000000e+02 0.00000000e+00 1.10000000e+01]\n",
            "   [2.05000000e+02 0.00000000e+00 1.10000000e+01]]\n",
            "\n",
            "  [[2.37010193e+02 2.40714279e+02 2.33790817e+02]\n",
            "   [2.24586731e+02 2.31872437e+02 2.25270401e+02]\n",
            "   [2.07260193e+02 2.19688766e+02 2.18331635e+02]\n",
            "   ...\n",
            "   [2.04357147e+02 7.85714149e-01 1.27857141e+01]\n",
            "   [2.04357147e+02 7.85714149e-01 1.27857141e+01]\n",
            "   [2.04357147e+02 7.85714149e-01 1.27857141e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.34000000e+02 2.01000000e+02 1.58000000e+02]\n",
            "   [2.34770432e+02 2.01770432e+02 1.58770432e+02]\n",
            "   [2.37045929e+02 2.04045929e+02 1.60617371e+02]\n",
            "   ...\n",
            "   [1.99785736e+02 2.09785736e+02 2.10785736e+02]\n",
            "   [2.00000000e+02 2.10000000e+02 2.11000000e+02]\n",
            "   [2.00000000e+02 2.10000000e+02 2.11000000e+02]]\n",
            "\n",
            "  [[2.34668365e+02 2.01668365e+02 1.58668365e+02]\n",
            "   [2.35994904e+02 2.02994904e+02 1.59994904e+02]\n",
            "   [2.36943863e+02 2.03943863e+02 1.60515289e+02]\n",
            "   ...\n",
            "   [2.00000000e+02 2.10000000e+02 2.11000000e+02]\n",
            "   [1.99862213e+02 2.09862213e+02 2.10862213e+02]\n",
            "   [1.98928558e+02 2.08928558e+02 2.09928558e+02]]\n",
            "\n",
            "  [[2.35127563e+02 2.02127563e+02 1.59127563e+02]\n",
            "   [2.33668335e+02 2.00668335e+02 1.57668335e+02]\n",
            "   [2.35214279e+02 2.02214279e+02 1.58785721e+02]\n",
            "   ...\n",
            "   [1.99137741e+02 2.09137741e+02 2.10137741e+02]\n",
            "   [1.97974487e+02 2.07974487e+02 2.08974487e+02]\n",
            "   [1.96872421e+02 2.06872421e+02 2.07872421e+02]]]\n",
            "\n",
            "\n",
            " [[[1.22959173e+00 3.22959185e+00 0.00000000e+00]\n",
            "   [6.42857194e-01 2.28571439e+00 0.00000000e+00]\n",
            "   [1.14795935e+00 3.07142878e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [3.08164740e+00 5.08164740e+00 4.08164740e+00]\n",
            "   [4.76021004e+00 6.76021004e+00 5.76021004e+00]\n",
            "   [4.14286137e+00 6.14286137e+00 5.14286137e+00]]\n",
            "\n",
            "  [[1.35714293e+00 3.35714293e+00 3.57142866e-01]\n",
            "   [2.00510192e+00 4.00510216e+00 1.00510204e+00]\n",
            "   [2.80102038e+00 4.80102015e+00 1.80102050e+00]\n",
            "   ...\n",
            "   [1.28572273e+00 3.28572273e+00 2.28572273e+00]\n",
            "   [3.79595089e+00 5.79595089e+00 4.79595089e+00]\n",
            "   [3.88265371e+00 5.88265371e+00 4.88265371e+00]]\n",
            "\n",
            "  [[0.00000000e+00 3.93367362e+00 3.14795947e+00]\n",
            "   [0.00000000e+00 3.37244940e+00 2.58673525e+00]\n",
            "   [0.00000000e+00 3.26020455e+00 2.47449040e+00]\n",
            "   ...\n",
            "   [1.64285755e+00 3.64285755e+00 2.64285755e+00]\n",
            "   [7.14268684e-01 2.71426868e+00 1.71426868e+00]\n",
            "   [7.65315965e-02 1.57143307e+00 5.71433067e-01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.31480389e+01 9.36230278e+00 1.01023209e+00]\n",
            "   [2.01989803e+01 6.41324520e+00 0.00000000e+00]\n",
            "   [2.46378288e+01 1.08520927e+01 1.85209239e+00]\n",
            "   ...\n",
            "   [1.30428680e+02 6.70000916e+01 3.30715637e+01]\n",
            "   [1.20627335e+02 5.56273384e+01 2.74845161e+01]\n",
            "   [1.14591858e+02 4.85918617e+01 2.41633339e+01]]\n",
            "\n",
            "  [[2.79081326e+01 1.09081335e+01 3.90813351e+00]\n",
            "   [3.08418350e+01 1.38418341e+01 6.84183407e+00]\n",
            "   [2.70458488e+01 1.00458488e+01 3.21424627e+00]\n",
            "   ...\n",
            "   [1.28698898e+02 6.60713196e+01 2.98570251e+01]\n",
            "   [1.29561142e+02 6.45662460e+01 3.42703400e+01]\n",
            "   [1.20826439e+02 5.48978767e+01 2.87549953e+01]]\n",
            "\n",
            "  [[2.90868626e+01 1.20868616e+01 5.08686209e+00]\n",
            "   [3.22602730e+01 1.42602711e+01 1.02602711e+01]\n",
            "   [2.22041702e+01 4.20417070e+00 1.21432018e+00]\n",
            "   ...\n",
            "   [1.22495056e+02 6.09287109e+01 1.77042179e+01]\n",
            "   [1.19173439e+02 5.51989441e+01 1.89796009e+01]\n",
            "   [1.23984993e+02 5.91125603e+01 2.75563450e+01]]]] and labels = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "## see an example of batches of our data\n",
        "for images, labels in train_data_10_percent.take(1):  ## 32 is the batch size by default hence the take will take 1 batch\n",
        "  print(f\"Images = {images} and labels = {labels}\")\n",
        "  print(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTA9oaF1xTtW"
      },
      "source": [
        "## Model 0: Using transfer learning to build a keras functional API\n",
        "### The Sequential API runs our layers in the model in sequential manner but functional API is more flexible and can take multiple inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOc8AvaXrLAw",
        "outputId": "2a0d8d82-a762-41b7-9bb3-205fa3c329bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n",
            "The shape of the base model after passing the inputs is (None, 7, 7, 1280)\n",
            "The shape of the model after globalaveragepool is (None, 1280)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
            "                                                                 \n",
            " global_average_pool_2d (Glo  (None, 1280)             0         \n",
            " balAveragePooling2D)                                            \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,062,381\n",
            "Trainable params: 12,810\n",
            "Non-trainable params: 4,049,571\n",
            "_________________________________________________________________\n",
            "moDEL SUMMARY = None\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
            "                                                                 \n",
            " global_average_pool_2d (Glo  (None, 1280)             0         \n",
            " balAveragePooling2D)                                            \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,062,381\n",
            "Trainable params: 12,810\n",
            "Non-trainable params: 4,049,571\n",
            "_________________________________________________________________\n",
            "moDEL SUMMARY = None\n",
            "Saving TensorBoard log files to: Transfer_learning/feature_extraction-10_percent/20221011-162328\n",
            "Epoch 1/3\n",
            "24/24 [==============================] - 542s 23s/step - loss: 1.9266 - accuracy: 0.3933 - val_loss: 1.3747 - val_accuracy: 0.6990\n",
            "Epoch 2/3\n",
            "24/24 [==============================] - 6s 225ms/step - loss: 1.1577 - accuracy: 0.7400 - val_loss: 0.9388 - val_accuracy: 0.7928\n",
            "Epoch 3/3\n",
            "24/24 [==============================] - 6s 221ms/step - loss: 0.8580 - accuracy: 0.7907 - val_loss: 0.7660 - val_accuracy: 0.8109\n"
          ]
        }
      ],
      "source": [
        "## Create base model with tf.keras.applications \n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "## Freeze the base model so that the underlying pretrained models are not retrained again\n",
        "base_model.trainable = False\n",
        "\n",
        "## create inputs to our model\n",
        "inputs = tf.keras.layers.Input(shape = (224,224,3), name = \"Input layer\")\n",
        "\n",
        "## If we are using a model like Resnet50V2 we will need to normalize inputs, but not for effiecientNet.\n",
        "# x = tf.keras.experimental.preprocessing.Rescaling(1./255)(inputs).  ## this means passing the inputs as parameter\n",
        "\n",
        "x = base_model(inputs)\n",
        "\n",
        "print(f\"The shape of the base model after passing the inputs is {x.shape}\")\n",
        "\n",
        "## Average pool the output of the base_model input layer\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name = \"global_average_pool_2d\") (x)\n",
        "print(f\"The shape of the model after globalaveragepool is {x.shape}\")\n",
        "\n",
        "## create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "## combine the input and output layers\n",
        "model_0 = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "## Let's check the summary of the model\n",
        "print(f\"moDEL SUMMARY = {model_0.summary()}\")\n",
        "\n",
        "## compiling the model\n",
        "model_0.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = [\"accuracy\"])\n",
        "\n",
        "## Let's check the summary of the model\n",
        "print(f\"moDEL SUMMARY = {model_0.summary()}\")\n",
        "\n",
        "## fitting the model\n",
        "from Helper import create_tensorboard_callback\n",
        "history_0 = model_0.fit(train_data_10_percent,epochs = 3,steps_per_epoch=len(train_data_10_percent),validation_data=test_data,\n",
        "                        validation_steps = int(0.25 * len(test_data)),callbacks = [create_tensorboard_callback(dir_name = \"Transfer_learning\",\n",
        "                                                                                                               experiment_name = \"feature_extraction-10_percent\")])\n",
        "\n",
        "\n",
        "## validation_steps = int(0.25 * len(test_data)) will reduce the validation steps to 1/4 and hence reduce training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lqriU0R20BH"
      },
      "outputs": [],
      "source": [
        "## Now let's evaluate on full test dataset\n",
        "model_0.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAXRpicE7K4C"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFayrdgb7mxw"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVi2N_uT7n42"
      },
      "outputs": [],
      "source": [
        "## lET'S CHECK OUR TRAINING CURVES\n",
        "from Helper import plot_loss_curves\n",
        "plot_loss_curves(history_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puExvirTU2Tb"
      },
      "source": [
        "## Running a series of transfer learning experments-:\n",
        "\n",
        "We have seen the transfer learning working incredibly with out 10% percent data. Now let's see how the model performs with the 1% of the dataset.\n",
        "\n",
        "1. `Model_1` : Use feature extraction transfer learning with 1% of the training dataset with data augmentation.\n",
        "2. `Model_2` :  Use feature extraction transfer learning with 10% of the training dataset with data augmentation.\n",
        "3. `Model_3` : Use fine tuning transfer learning with 10% of the training data with data augmentation.\n",
        "4. `Model_4` : Use fine tuning transfer learning with 10% of the training data with data augmentation.\n",
        "\n",
        "\n",
        "We will use same datset in all the model experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV8QI29Cc_2b"
      },
      "source": [
        "## getting and preprocessing the data for model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgfTiXgF8V2Y",
        "outputId": "bce3566c-4e3c-428b-cc34-0dd596a07b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data set is already unzipped and present\n"
          ]
        }
      ],
      "source": [
        "## Let's get the 1 percent data\n",
        "from Helper import unzip_data\n",
        "try:\n",
        "  if os.path.exists(os.path.join(os.getcwd(),\"10_food_classes_1_percent.zip\")):\n",
        "    print(f\"data set is already unzipped and present\")\n",
        "  else:\n",
        "    print(\"Downloading the data and unzipping it...\")\n",
        "    !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "    ### unzipping data\n",
        "    print(\"Unzipping the data\")\n",
        "    unzip_data(\"10_food_classes_1_percent.zip\")\n",
        "\n",
        "\n",
        "    \n",
        "except Exception as e:\n",
        "  print(\"data not found in the directory... Hence downloading.....\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqNGH82jcAz9"
      },
      "outputs": [],
      "source": [
        "## creating training and testing directories\n",
        "train_dir_1_percent = \"10_food_classes_1_percent/train\" \n",
        "test_dir = \"10_food_classes_1_percent/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9DnJ2T7cSNO",
        "outputId": "9118caaa-b0c6-4bc3-8bd6-e5534e0e1d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_1_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_1_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_1_percent/test/fried_rice'.\n",
            "There are 10 directories and 0 images in '10_food_classes_1_percent/train'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/ice_cream'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/chicken_curry'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/steak'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/sushi'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/chicken_wings'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/hamburger'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/pizza'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/ramen'.\n",
            "There are 0 directories and 7 images in '10_food_classes_1_percent/train/fried_rice'.\n"
          ]
        }
      ],
      "source": [
        "## walkthrough the directory\n",
        "from Helper import walk_through_dir\n",
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6P-2OvicxoL",
        "outputId": "7436a14d-a0fe-414c-9444-f77d46e16e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 70 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "## Setting up the dataloaders\n",
        "\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n",
        "                                                                                 label_mode = \"categorical\",\n",
        "                                                                                 image_size = IMG_SIZE,\n",
        "                                                                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                                      label_mode = \"categorical\",\n",
        "                                                                      image_size = IMG_SIZE,\n",
        "                                                                      batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVKR0gL_eG8h",
        "outputId": "b7c447da-ef69-4e5b-803b-f87dc8e1e90d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v8qF5SN3ygX"
      },
      "source": [
        "## Adding data augmentation right into the model\n",
        "### We can use `tf.keras.layers.experimental.preprocessing`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oQTf1yqceYnO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "## Let's create a data augmentation stage with horizontal flipping, rotation and zoom etc\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomHeight(0.2)\n",
        "    \n",
        "], name = \"data_augmentation_layer\")\n",
        "\n",
        "## The advantage of augmenting data on the stage level rather than image_data_generator level is that stage will \n",
        "## be considered in model training and hence it will use gpu but image_data_generator will use cpu ONLY."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_augmentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWVmbgC9Qbqt",
        "outputId": "f4fe9033-edc3-4396-cc8f-71c46a3a3a62"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f3b53053210>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwTXmZ10QvCP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XIlaiqpgtqqkx348CdOGRemPCCi0k9ru",
      "authorship_tag": "ABX9TyPxd3oI/h5xo3Ag/QRxOGOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}